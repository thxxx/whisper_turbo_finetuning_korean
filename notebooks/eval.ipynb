{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a567af8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'get_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mget_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_dataset\n\u001b[1;32m      3\u001b[0m tds, vds \u001b[38;5;241m=\u001b[39m get_dataset()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'get_data'"
     ]
    }
   ],
   "source": [
    "from get_data import get_dataset\n",
    "\n",
    "tds, vds = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391f32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khj6051/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a4399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'transcripts'],\n",
       "    num_rows: 160859\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e41ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2367b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "for i  in range(100, 200):\n",
    "    x  = ds[i][\"audio\"][\"array\"]\n",
    "    sr = ds[i][\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "    print(ds[i]['transcripts'])\n",
    "    # display(Audio(x, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.cache_files['test'][0]['filename']+\"/0004.wav\")\n",
    "import os\n",
    "\n",
    "os.path.exists(ds.cache_files['test'][0]['filename']+\"/0004.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67306cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ds.train_test_split(test_size=0.1, seed=42)  # seed 고정하면 재현성 O\n",
    "\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"]\n",
    "\n",
    "print(len(train_ds), len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"kresnik/zeroth_korean\")[\"train\"]\n",
    "splits = ds.train_test_split(test_size=0.1, seed=42)  # seed 고정하면 재현성 O\n",
    "\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"][:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a498b47",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63046d07",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Junhoee/STT_Korean_Dataset_80000\")\n",
    "\n",
    "data = next(iter(ds['train']))\n",
    "print(data['audio'])\n",
    "print(data['transcripts'])\n",
    "\n",
    "data['audio']['array'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df15d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "path = \"Emilia/KO/*.tar\" # Same for Emilia-YODAS; just replace \"Emilia/\" with \"Emilia-YODAS/\"\n",
    "dataset = load_dataset(\"amphion/Emilia-Dataset\", data_files={\"ko\": path}, split=\"ko\", streaming=False)\n",
    "splits = dataset.train_test_split(test_size=0.1, seed=42)  # seed 고정하면 재현성 O\n",
    "\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44facf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a2d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "\n",
    "audios = []\n",
    "refs = []\n",
    "for s in batch_samples:\n",
    "    audio_np, ref_text = preprocess_sample(s)\n",
    "    audios.append(audio_np.to(device))\n",
    "    refs.append(ref_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "results = pipe(\n",
    "    audios[0],\n",
    ")\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da12797",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    for batch_samples in tqdm(chunked(limited_iter, BATCH), desc=\"Batched ASR\"):\n",
    "        # 전처리\n",
    "        audios: List[np.ndarray] = []\n",
    "        refs: List[str] = []\n",
    "        for s in batch_samples:\n",
    "            audio_np, ref_text = preprocess_sample(s)\n",
    "            audios.append(audio_np)\n",
    "            refs.append(ref_text)\n",
    "\n",
    "        # 파이프라인에 리스트로 입력 (배치 추론)\n",
    "        # 긴 파일 대비 안전하게 chunk/stride 설정\n",
    "        results = pipe(\n",
    "            audios,\n",
    "            batch_size=BATCH,\n",
    "            return_timestamps=False\n",
    "        )\n",
    "        # results: List[{'text': ...}, ...]\n",
    "        hyps = [r[\"text\"] for r in results]\n",
    "\n",
    "        # 메트릭 누적\n",
    "        for hyp, ref in zip(hyps, refs):\n",
    "            wers.append(wer(ref, hyp, unit=\"word\"))\n",
    "            cers.append(cer(ref, hyp))\n",
    "\n",
    "        total_count += len(batch_samples)\n",
    "\n",
    "        # 주기 출력/저장\n",
    "        if total_count % 500 < BATCH:  # 500단위 근처에서 한 번\n",
    "            avg_wer = sum(wers) / len(wers) if wers else 0.0\n",
    "            avg_cer = sum(cers) / len(cers) if cers else 0.0\n",
    "            line = f\"[{total_count}th] - WER: {avg_wer:.6f}, CER: {avg_cer:.6f}\"\n",
    "            print(line)\n",
    "            with open(\"wer.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "# 최종 기록\n",
    "final_wer = sum(wers) / len(wers) if wers else 0.0\n",
    "final_cer = sum(cers) / len(cers) if cers else 0.0\n",
    "with open(\"wer.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"[{total_count}th] - WER: {final_wer:.6f}, CER: {final_cer:.6f}\\n\")\n",
    "\n",
    "print(f\"DONE: N={total_count}, WER={final_wer:.6f}, CER={final_cer:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f64f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fed65b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 10010.27it/s]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import librosa\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ab560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9684c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443647\n",
      "418571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2081.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from utils import clean_transcript\n",
    "\n",
    "ds = load_dataset(\"idiotDeveloper/koreanTelephone\")\n",
    "\n",
    "ds = ds.cast_column(\"audio\", Audio(decode=True, sampling_rate=16000))\n",
    "\n",
    "print(len(ds['test']))\n",
    "arr = np.array(ds['test'][\"transcripts\"])\n",
    "mask = np.array([not (\"((\" in t or \"))\" in t) for t in arr])\n",
    "ds['test'] = ds['test'].select(np.where(mask)[0])\n",
    "print(len(ds['test']))\n",
    "\n",
    "ds = ds[\"test\"].select(range(100)).map(clean_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a503ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "for idx, data in tqdm(enumerate(ds)):\n",
    "    if idx<10:\n",
    "        continue\n",
    "        \n",
    "    audio = torch.tensor(data['audio']['array']).to(device)\n",
    "    result = pipe(audio)\n",
    "    display(Audio(audio.cpu().numpy(), rate=16000))\n",
    "    print(data['transcripts'])\n",
    "    print(result['text'])\n",
    "\n",
    "    if idx > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292dc53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
