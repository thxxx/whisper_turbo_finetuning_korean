{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a567af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from datasets.tasks.base import dataclass\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# ds = load_dataset(\"o0dimplz0o/Zeroth-STT-Korean\")\n",
    "ds = load_dataset(\"idiotDeveloper/koreanTelephone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6df58ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:13<00:00, 717.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tss = set([])\n",
    "for i in tqdm(range(10000)):\n",
    "    tss.add(ds['test'][i]['transcripts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb34e68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76428b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from utils import clean_transcript\n",
    "\n",
    "for i in range(50, 80):\n",
    "    print(ds3['train'][i]['transcripts'])\n",
    "    display(Audio(ds3['train'][i]['audio']['array'], rate=ds3['train'][i]['audio']['sampling_rate']))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e41ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds3 = ds3.cast_column(\"audio\", Audio(decode=True, sampling_rate=16000))\n",
    "arr = np.array(ds3['test'][\"transcripts\"])\n",
    "mask = np.array([not (\"((\" in t or \"))\" in t) for t in arr])\n",
    "ds3['test'] = ds3['test'].select(np.where(mask)[0])\n",
    "ds3['test'] = ds3['test'].map(clean_transcript)\n",
    "\n",
    "def remove_duplicates(ds, key=\"text\"):\n",
    "    seen = set()\n",
    "    new_data = []\n",
    "    for ex in ds:\n",
    "        val = ex[key]\n",
    "        if val not in seen:\n",
    "            seen.add(val)\n",
    "            new_data.append(ex)\n",
    "    return Dataset.from_list(new_data)\n",
    "\n",
    "print(len(ds3['test']))\n",
    "ds_unique = remove_duplicates(ds3['test'], key=\"transcripts\")\n",
    "print(len(ds_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff88eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   5%|▌         | 173000/3348572 [02:05<1:39:15, 533.19 examples/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from utils import clean_transcript\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc  # ✨ Arrow로 dedup\n",
    "\n",
    "arr = np.array(ds[\"transcripts\"])\n",
    "mask = np.array([not (\"((\" in t or \"))\" in t) for t in arr])\n",
    "ds = ds.select(np.where(mask)[0])\n",
    "ds = ds.rename_column(\"transcripts\", \"text\")\n",
    "ds = ds.map(clean_transcript)\n",
    "\n",
    "print(len(ds))\n",
    "\n",
    "def dedup_by_text(ds, col=\"transcripts\"):\n",
    "    seen = set()\n",
    "    keep_idx = []\n",
    "    for i, t in enumerate(ds[col]):\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            keep_idx.append(i)\n",
    "    return ds.select(keep_idx)\n",
    "\n",
    "ds = dedup_by_text(ds)\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebfe12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376774\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d41f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2367b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "for i  in range(100, 200):\n",
    "    x  = ds[i][\"audio\"][\"array\"]\n",
    "    sr = ds[i][\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "    print(ds[i]['transcripts'])\n",
    "    # display(Audio(x, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.cache_files['test'][0]['filename']+\"/0004.wav\")\n",
    "import os\n",
    "\n",
    "os.path.exists(ds.cache_files['test'][0]['filename']+\"/0004.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67306cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ds.train_test_split(test_size=0.1, seed=42)  # seed 고정하면 재현성 O\n",
    "\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"]\n",
    "\n",
    "print(len(train_ds), len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"kresnik/zeroth_korean\")[\"train\"]\n",
    "splits = ds.train_test_split(test_size=0.1, seed=42)  # seed 고정하면 재현성 O\n",
    "\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"][:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a498b47",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63046d07",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Junhoee/STT_Korean_Dataset_80000\")\n",
    "\n",
    "data = next(iter(ds['train']))\n",
    "print(data['audio'])\n",
    "print(data['transcripts'])\n",
    "\n",
    "data['audio']['array'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df15d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "path = \"Emilia/KO/*.tar\" # Same for Emilia-YODAS; just replace \"Emilia/\" with \"Emilia-YODAS/\"\n",
    "dataset = load_dataset(\"amphion/Emilia-Dataset\", data_files={\"ko\": path}, split=\"ko\", streaming=False)\n",
    "splits = dataset.train_test_split(test_size=0.1, seed=42)  # seed 고정하면 재현성 O\n",
    "\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44facf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a2d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "\n",
    "audios = []\n",
    "refs = []\n",
    "for s in batch_samples:\n",
    "    audio_np, ref_text = preprocess_sample(s)\n",
    "    audios.append(audio_np.to(device))\n",
    "    refs.append(ref_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "results = pipe(\n",
    "    audios[0],\n",
    ")\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da12797",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    for batch_samples in tqdm(chunked(limited_iter, BATCH), desc=\"Batched ASR\"):\n",
    "        # 전처리\n",
    "        audios: List[np.ndarray] = []\n",
    "        refs: List[str] = []\n",
    "        for s in batch_samples:\n",
    "            audio_np, ref_text = preprocess_sample(s)\n",
    "            audios.append(audio_np)\n",
    "            refs.append(ref_text)\n",
    "\n",
    "        # 파이프라인에 리스트로 입력 (배치 추론)\n",
    "        # 긴 파일 대비 안전하게 chunk/stride 설정\n",
    "        results = pipe(\n",
    "            audios,\n",
    "            batch_size=BATCH,\n",
    "            return_timestamps=False\n",
    "        )\n",
    "        # results: List[{'text': ...}, ...]\n",
    "        hyps = [r[\"text\"] for r in results]\n",
    "\n",
    "        # 메트릭 누적\n",
    "        for hyp, ref in zip(hyps, refs):\n",
    "            wers.append(wer(ref, hyp, unit=\"word\"))\n",
    "            cers.append(cer(ref, hyp))\n",
    "\n",
    "        total_count += len(batch_samples)\n",
    "\n",
    "        # 주기 출력/저장\n",
    "        if total_count % 500 < BATCH:  # 500단위 근처에서 한 번\n",
    "            avg_wer = sum(wers) / len(wers) if wers else 0.0\n",
    "            avg_cer = sum(cers) / len(cers) if cers else 0.0\n",
    "            line = f\"[{total_count}th] - WER: {avg_wer:.6f}, CER: {avg_cer:.6f}\"\n",
    "            print(line)\n",
    "            with open(\"wer.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "# 최종 기록\n",
    "final_wer = sum(wers) / len(wers) if wers else 0.0\n",
    "final_cer = sum(cers) / len(cers) if cers else 0.0\n",
    "with open(\"wer.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"[{total_count}th] - WER: {final_wer:.6f}, CER: {final_cer:.6f}\\n\")\n",
    "\n",
    "print(f\"DONE: N={total_count}, WER={final_wer:.6f}, CER={final_cer:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f64f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed65b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import librosa\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ab560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9684c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from utils import clean_transcript\n",
    "\n",
    "ds = load_dataset(\"idiotDeveloper/koreanTelephone\")\n",
    "\n",
    "ds = ds.cast_column(\"audio\", Audio(decode=True, sampling_rate=16000))\n",
    "\n",
    "print(len(ds['test']))\n",
    "arr = np.array(ds['test'][\"transcripts\"])\n",
    "mask = np.array([not (\"((\" in t or \"))\" in t) for t in arr])\n",
    "ds['test'] = ds['test'].select(np.where(mask)[0])\n",
    "print(len(ds['test']))\n",
    "\n",
    "ds = ds[\"test\"].select(range(100)).map(clean_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a503ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "for idx, data in tqdm(enumerate(ds)):\n",
    "    if idx<10:\n",
    "        continue\n",
    "        \n",
    "    audio = torch.tensor(data['audio']['array']).to(device)\n",
    "    result = pipe(audio)\n",
    "    display(Audio(audio.cpu().numpy(), rate=16000))\n",
    "    print(data['transcripts'])\n",
    "    print(result['text'])\n",
    "\n",
    "    if idx > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292dc53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
